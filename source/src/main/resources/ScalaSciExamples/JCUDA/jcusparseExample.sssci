import  jcuda.jcusparse.JCusparse._
import  jcuda.jcusparse.cusparseIndexBase.CUSPARSE_INDEX_BASE_ZERO
import jcuda.jcusparse.cusparseMatrixType.CUSPARSE_MATRIX_TYPE_GENERAL
import jcuda.jcusparse.cusparseOperation.CUSPARSE_OPERATION_NON_TRANSPOSE
import jcuda.runtime.JCuda._
import jcuda.runtime.cudaMemcpyKind._
import jcuda._
import jcuda.jcusparse._
import jcuda.runtime.JCuda


/**
 * A sample application showing how to use JCusparse.<br />
 * <br />
 * This sample has been ported from the NVIDIA CURAND 
 * documentation example. 
 */
 
        // Enable exceptions and subsequently omit error checks in this sample
        JCusparse.setExceptionsEnabled(true)
        JCuda.setExceptionsEnabled(true)
        
        // Variable declarations
        var handle = new cusparseHandle()
        var descra = new cusparseMatDescr()
        
        var  cooRowIndexHostPtr = new Array[Int](1)
        var  cooColIndexHostPtr = new Array[Int](1)
        var  cooValHostPtr = new Array[Float](1)
        
        var  cooRowIndex = new Pointer()
        var  cooColIndex = new Pointer()
        var  cooVal = new Pointer()
        
        var  xIndHostPtr = new Array [Int](1)
        var  xValHostPtr = new Array[Float](1)
        var  yHostPtr = new Array[Float](1)
        
        var  xInd = new Pointer()
        var  xVal = new Pointer()
        var  y = new Pointer()
        var csrRowPtr = new Pointer()
        
        var  zHostPtr = new Array[Float](1)
        var z = new Pointer()
        
        println("Testing example")
        // Create the following sparse test matrix in COO format
        // | 1.0      2.0   3.0 |
        // |     4.0                |
        // | 5.0      6.0  7.0  |
        // |     8.0         9.0  |
       var n = 4
        var nnz = 9  // # of non-zero elements
        cooRowIndexHostPtr = new Array[Int](nnz)
        cooColIndexHostPtr = new Array[Int](nnz)
        cooValHostPtr      = new Array[Float](nnz)
        
        cooRowIndexHostPtr(0)=0; cooColIndexHostPtr(0)=0; cooValHostPtr(0)=1.0f;
        cooRowIndexHostPtr(1)=0; cooColIndexHostPtr(1)=2; cooValHostPtr(1)=2.0f;
        cooRowIndexHostPtr(2)=0; cooColIndexHostPtr(2)=3; cooValHostPtr(2)=3.0f;
        cooRowIndexHostPtr(3)=1; cooColIndexHostPtr(3)=1; cooValHostPtr(3)=4.0f;
        cooRowIndexHostPtr(4)=2; cooColIndexHostPtr(4)=0; cooValHostPtr(4)=5.0f;
        cooRowIndexHostPtr(5)=2; cooColIndexHostPtr(5)=2; cooValHostPtr(5)=6.0f;
        cooRowIndexHostPtr(6)=2; cooColIndexHostPtr(6)=3; cooValHostPtr(6)=7.0f;
        cooRowIndexHostPtr(7)=3; cooColIndexHostPtr(7)=1; cooValHostPtr(7)=8.0f;
        cooRowIndexHostPtr(8)=3; cooColIndexHostPtr(8)=3; cooValHostPtr(8)=9.0f;
        
        // Create a sparse and a dense vector
        // xVal=[100.0, 200.0, 400.0] (sparse)
        // xInd=[0      1      3    ]
        // y   =[10.0, 20.0, 30.0, 40.0 | 50.0, 60.0, 70.0, 80.0] (dense)
        var nnz_vector = 3
        n = 4
        xIndHostPtr = new Array[Int](nnz_vector)
        xValHostPtr = new Array[Float](nnz_vector)
        yHostPtr    = new Array[Float](2*n)
        zHostPtr    = new Array[Float](2*(n+1))
        
        yHostPtr(0) = 10.0f;  xIndHostPtr(0)=0; xValHostPtr(0)=100.0f;
        yHostPtr(1) = 20.0f;  xIndHostPtr(1)=1; xValHostPtr(1)=200.0f;
        yHostPtr(2) = 30.0f;  
        yHostPtr(3) = 40.0f;  xIndHostPtr(2)=3; xValHostPtr(2)=400.0f;
        yHostPtr(4) = 50.0f;  
        yHostPtr(5) = 60.0f;  
        yHostPtr(6) = 70.0f;  
        yHostPtr(7) = 80.0f;  
        
        
        // Allocate GPU memory and copy the matrix and vectors into it
        cudaMalloc(cooRowIndex, nnz*Sizeof.INT)
        cudaMalloc(cooColIndex, nnz*Sizeof.INT)
        cudaMalloc(cooVal,      nnz*Sizeof.FLOAT)
        cudaMalloc(y,           2*n*Sizeof.FLOAT)
        cudaMalloc(xInd,        nnz_vector*Sizeof.INT)
        cudaMalloc(xVal,        nnz_vector*Sizeof.FLOAT)
        cudaMemcpy(cooRowIndex, Pointer.to(cooRowIndexHostPtr), nnz*Sizeof.INT,          cudaMemcpyHostToDevice)
        cudaMemcpy(cooColIndex, Pointer.to(cooColIndexHostPtr), nnz*Sizeof.INT,          cudaMemcpyHostToDevice)
        cudaMemcpy(cooVal,      Pointer.to(cooValHostPtr),      nnz*Sizeof.FLOAT,        cudaMemcpyHostToDevice)
        cudaMemcpy(y,           Pointer.to(yHostPtr),           2*n*Sizeof.FLOAT,        cudaMemcpyHostToDevice)
        cudaMemcpy(xInd,        Pointer.to(xIndHostPtr),        nnz_vector*Sizeof.INT,   cudaMemcpyHostToDevice)
        cudaMemcpy(xVal,        Pointer.to(xValHostPtr),        nnz_vector*Sizeof.FLOAT, cudaMemcpyHostToDevice)

        // Initialize JCusparse library
        cusparseCreate(handle)
        
        // Create and set up matrix descriptor
        cusparseCreateMatDescr(descra)
        cusparseSetMatType(descra, CUSPARSE_MATRIX_TYPE_GENERAL)
        cusparseSetMatIndexBase(descra, CUSPARSE_INDEX_BASE_ZERO)
        
        // Exercise conversion routines (convert matrix from COO 2 CSR format)
        cudaMalloc(csrRowPtr, (n+1)*Sizeof.INT)
        cusparseXcoo2csr(handle, cooRowIndex, nnz, n,  csrRowPtr, CUSPARSE_INDEX_BASE_ZERO)
        //csrRowPtr = [0 3 4 7 9]

        // Exercise Level 1 routines (scatter vector elements)
        var  yn = y.withByteOffset(n*Sizeof.FLOAT)
        cusparseSsctr(handle, nnz_vector, xVal, xInd,  yn, CUSPARSE_INDEX_BASE_ZERO)
        // y = [10 20 30 40 | 100 200 70 400]
        
        // Exercise Level 2 routines (csrmv)
        var  y0 = y.withByteOffset(0)
        cusparseScsrmv(handle, CUSPARSE_OPERATION_NON_TRANSPOSE, n, n, 2.0f, 
          descra, cooVal, csrRowPtr, cooColIndex, y0, 3.0f, yn)
        
        // Print intermediate results (y)
        // y = [10 20 30 40 | 680 760 1230 2240]
        cudaMemcpy(Pointer.to(yHostPtr), y, 2*n*Sizeof.FLOAT, cudaMemcpyDeviceToHost)
         var j=0; var i=0
         while  (j<  2)
        {
                i = 0
                
           while  (i< n)
            {
                 println("yHostPtr["+i+","+j+"] = " + yHostPtr(i+n*j))
                 i += 1
            }
            j+=1
        }
        
        // Exercise Level 3 routines (csrmm) 
        cudaMalloc(z, 2*(n+1)*Sizeof.FLOAT)
        cudaMemset(z, 0, 2*(n+1)*Sizeof.FLOAT)
        cusparseScsrmm(handle, CUSPARSE_OPERATION_NON_TRANSPOSE, n, 2, n, 
            5.0f, descra, cooVal, csrRowPtr, cooColIndex, y, n, 0.0f, z, n+1)
        
        // Print final results (z)
        // z = [950 400 2550 2600 0 | 49300 15200 132300 131200 0]
        cudaMemcpy(Pointer.to(zHostPtr), z, 2*(n+1)*Sizeof.FLOAT, cudaMemcpyDeviceToHost)
        
        println("Final results:\n")
         j=0
         i=0
        while (j < 2)
        {
            i = 0
            while (i < n-1)
            {
                println("z["+i+","+j+ "]= "+ zHostPtr(i+(n+1)*j))
                i += 1
            }
            j += 1
        }
        
        // Clean up
        cudaFree(y);
        cudaFree(z);
        cudaFree(xInd);
        cudaFree(xVal);
        cudaFree(csrRowPtr);
        cudaFree(cooRowIndex);
        cudaFree(cooColIndex);
        cudaFree(cooVal);
        cusparseDestroy(handle);
    